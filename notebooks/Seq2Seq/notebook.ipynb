{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German -> English\n",
    "\n",
    "https://www.youtube.com/watch?v=EoGUlvhRYpk&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
    "from utils import translate_sentence, bleu, save_checkpoint, load_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_ger = spacy.load('de')\n",
    "spacy_eng = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_ger(text):\n",
    "    return [tok.text for tok in spacy_ger.tokenizer(text)]\n",
    "\n",
    "def tokenizer_eng(text):\n",
    "    return [tok.text for tok in spacy_eng.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "german = Field(tokenize=tokenizer_ger, lower=True, init_token='<sos>', eos_token='<eos>')\n",
    "english = Field(tokenize=tokenizer_eng, lower=True, init_token='<sos>', eos_token='<eos>')\n",
    "\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(german, english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "german.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "english.build_vocab(train_data, max_size=10000, min_freq=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "        super(Encoder, self).__init__()        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_length, N)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (seq_length, N, embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "        # outputs shape: (seq_length, N, hidden_size)\n",
    "\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x shape: (N) but we want (1, N)\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (1, N, embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        # output shape: (1, N, hidden_size)\n",
    "\n",
    "        predictions = self.fc(outputs)\n",
    "        # predictions shape: (1, N, length_of_vocab)\n",
    "        \n",
    "        predictions = predictions.squeeze(0)\n",
    "\n",
    "        return predictions, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(english.vocab)\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "\n",
    "        hidden, cell = self.encoder(source)\n",
    "\n",
    "        # Grab start token\n",
    "        x = target[0]\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "            \n",
    "            outputs[t] = output\n",
    "            # output shape: (N, english_vocab_size)\n",
    "\n",
    "            best_guess = output.argmax(1)\n",
    "            \n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "# Model hyperparameters\n",
    "load_model = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size_encoder = len(german.vocab)\n",
    "input_size_decoder = len(english.vocab)\n",
    "output_size = len(english.vocab)\n",
    "\n",
    "# embeeding_size of around 100-300 is good number\n",
    "# based on size of the dataset\n",
    "encoder_embedding_size = 300\n",
    "decoder_embedding_size = 300\n",
    "\n",
    "hidden_size = 1024\n",
    "num_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "\n",
    "# Tensorboard\n",
    "writer = SummaryWriter(f'runs/loss_plot')\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = batch_size,\n",
    "    sort_within_batch = True,\n",
    "    sort_key = lambda x: len(x.src),\n",
    "    device = device\n",
    ")\n",
    "\n",
    "encoder_net = Encoder(\n",
    "    input_size_encoder,\n",
    "    encoder_embedding_size,\n",
    "    hidden_size,\n",
    "    num_layers,\n",
    "    enc_dropout\n",
    ").to(device)\n",
    "\n",
    "decoder_net = Decoder(\n",
    "    input_size_decoder,\n",
    "    decoder_embedding_size,\n",
    "    hidden_size,\n",
    "    output_size,\n",
    "    num_layers,\n",
    "    dec_dropout\n",
    ").to(device)\n",
    "\n",
    "model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "pad_idx = english.vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "sentence = \"ein boot mit mehreren männern darauf wird von einem großen pferdegespann ans ufer gezogen.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoc [0 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence \n",
      " ['a', '<unk>', 'player', 'in', 'a', 'red', 'shirt', 'is', 'a', 'the', 'ball', 'of', 'a', '<unk>', '.', '<eos>']\n",
      "\n",
      "Epoc [1 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence \n",
      " ['a', 'large', 'player', 'with', 'a', '<unk>', 'with', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "\n",
      "Epoc [2 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence \n",
      " ['a', 'white', 'player', 'with', 'a', '<unk>', 'is', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "\n",
      "Epoc [3 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence \n",
      " ['a', 'man', 'with', 'a', 'number', 'is', 'is', 'to', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "\n",
      "Epoc [4 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence \n",
      " ['a', 'boat', 'with', 'with', 'a', 'number', 'of', 'a', 'from', 'a', 'large', 'of', 'a', '.', '<eos>']\n",
      "\n",
      "Epoc [5 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence \n",
      " ['a', 'boat', 'with', 'a', 'number', 'is', 'being', 'pulled', 'by', 'a', 'large', 'building', 'of', 'a', '.', '<eos>']\n",
      "\n",
      "Epoc [6 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence \n",
      " ['a', 'boat', 'with', 'a', 'number', 'is', 'being', 'pulled', 'by', 'a', 'large', 'from', 'a', 'large', 'building', '.', '<eos>']\n",
      "\n",
      "Epoc [7 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence \n",
      " ['a', 'boat', 'with', 'a', 'number', 'is', 'pulled', 'from', 'from', 'a', 'large', 'by', 'a', 'large', '.', '.', '<eos>']\n",
      "\n",
      "Epoc [8 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence \n",
      " ['a', 'boat', 'with', 'several', 'dogs', 'pulled', 'off', 'of', 'a', 'off', 'a', 'a', 'by', 'a', 'large', '.', '.', '<eos>']\n",
      "\n",
      "Epoc [9 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence \n",
      " ['a', 'boat', 'with', 'a', 'men', 'is', 'being', 'pulled', 'by', 'a', 'large', 'by', 'a', 'large', '.', '<eos>']\n",
      "\n",
      "Epoc [10 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence \n",
      " ['a', 'man', 'with', 'two', 'men', 'on', 'a', 'from', 'a', 'large', ',', 'two', 'two', 'men', '.', '<eos>']\n",
      "\n",
      "Epoc [11 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence \n",
      " ['a', 'boat', 'with', 'two', 'men', 'men', 'pulled', 'from', 'a', 'large', 'group', 'of', 'a', 'large', 'cable', '.', '<eos>']\n",
      "\n",
      "Epoc [12 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence \n",
      " ['a', 'boat', 'with', 'two', 'men', 'of', 'pulled', 'by', 'a', 'large', 'of', 'horses', 'from', 'a', 'large', '.', '.', '<eos>']\n",
      "\n",
      "Epoc [13 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence \n",
      " ['a', 'boat', 'with', '3', 'men', 'is', 'pulled', 'by', 'a', 'large', 'pulled', 'by', 'large', 'large', '.', '<eos>']\n",
      "\n",
      "Epoc [14 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence \n",
      " ['a', 'boat', 'with', 'two', 'men', 'pulled', 'pulled', 'by', 'a', 'large', 'cable', 'by', 'two', 'large', '.', '.', '<eos>']\n",
      "\n",
      "Epoc [15 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence \n",
      " ['a', 'man', 'with', 'several', 'men', 'is', 'being', 'thrown', 'by', 'a', 'group', 'of', 'horses', '.', '<eos>']\n",
      "\n",
      "Epoc [16 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence \n",
      " ['a', 'boat', 'with', 'two', 'men', 'is', 'pulled', 'by', 'a', 'large', 'group', 'of', 'horses', 'by', 'a', 'large', '.', '<eos>']\n",
      "\n",
      "Epoc [17 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence \n",
      " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'pulled', 'by', 'a', 'large', 'group', 'of', 'horses', '.', '<eos>']\n",
      "\n",
      "Epoc [18 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence \n",
      " ['a', 'boat', 'with', 'several', 'men', 'is', 'being', 'pulled', 'by', 'a', 'large', 'group', 'of', 'boats', '.', '<eos>']\n",
      "\n",
      "Epoc [19 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence \n",
      " ['a', 'boat', 'with', 'several', 'men', 'is', 'stopped', 'by', 'shore', 'by', 'a', 'large', 'pile', 'of', 'horses', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "if load_model:\n",
    "    load_checkpoint(torch.load('my_checkpoint.pth.tar'), model, optimizer)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\nEpoc [{epoch} / {num_epochs}]')\n",
    "\n",
    "    checkpoint = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "    save_checkpoint(checkpoint)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    translated_sentence = translate_sentence(model, sentence, german, english, device, max_length=50)\n",
    "\n",
    "    print(f'Translated example sentence \\n {translated_sentence}')\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        input_data = batch.src.to(device)\n",
    "        target = batch.trg.to(device)\n",
    "\n",
    "        output = model(input_data, target)\n",
    "        # output shape: (trg_len, batch_size, output_dim)\n",
    "\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # to avoid gradient becomes too large\n",
    "        # make sure that gradients are in healthy range\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        writer.add_scalar('Training loss', loss, global_step=step)\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu score 17.73\n"
     ]
    }
   ],
   "source": [
    "score = bleu(test_data[1:100], model, german, english, device)\n",
    "print(f\"Bleu score {score*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
