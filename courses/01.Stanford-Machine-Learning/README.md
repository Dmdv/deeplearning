### Week 1

- Linear Regression with one variable
- Loss Function
- Parameter Learning

### Week 2

- Linear Regression with multiple variable
- Gradient Descent  (optimization algorithm)
- Feature Scaling and Learning Rate
- Normal Equation (final optimal minima without iteration like gradient descent)

### Week 3

- Classification with Logistic Regression
- Sigmoid (activation function)
- Logistic Regression Loss Function
- Simplified Gradient Descent for both Probability of class 0 and class 1
- Multiclass Classification (Logistic Regression) with One-vs-all
- Overfitting Problem
- Regularization method (to prevent overfitting)

### Week 4

- Neural Network - Model Representation (Input, Hidden, Output Layers)
- Neural Network - Forward Propagation

### Week 5

- Cost function in Neural Network
- Neural Network - Forward Propagation (to adjust the weights - Theta)
- Gradient checking - to make sure calculating error of Backward Propagation is working correctly
- Random initial theta initialization (Symmetry breaking)
- Neural Network Architecture (how to choose no. of input units (features), no. of output units (classes), no. of hidden units per layer)

### Week 6

- Evaluating Hypothesis (Test set error, Misclassification error for classification problems)
- Model Selection (Training, cross validation and Test set)
- Identify underfitting (high bias) or overfitting (high variance) by looking Training and Test error
- Choosing correct regularization parameter, based on training and cross-validation error
- Identify high bias or variance problem plotting training and cross-validation error on Learning Curves
- Precision and Recall as error metric for skewed classes

    > high precision and high recall is considered a good classifier.
    using F-score to evaluate precision and recall for good classifier.

### Week 7

- Support Vector Machine (SVM)
- SVM with Kernels for nonlinear classifiers

### Week 8

- Unsupervised Machine Learning
- K-means clustering algorithm
- Random initialization to set initial cluster
- Choosing number of clusters, i.e. Elbow method
- Principal Component Analysis (for dimensionality reduction of data, aka data compression)

### Week 9

- Gaussian Distribution
- Anomaly Detection Algorithm
- How to select features for anomaly detection
- Multivariant Gaussian Distribution

### Week 10

- using Gradient Descent optimization algorithm with large datasets
- Stochastic Gradient Descent
- Batch Gradient Descent
- Online Learning to continuously learning from the streaming data
- Map Reduce to train data in parallel

### Week 11

- Data pipeline to use multiple classifiers together
- OCR application example
    - Text detection with Sliding Window algorithm
    - Character segmentation with supervised learning
    - Character classification
- Artificial Data Synthesis
    - adding noise
    - distorting the image
- Ceiling Analysis
    - defining what part of the machine learning pipeline to work on next
